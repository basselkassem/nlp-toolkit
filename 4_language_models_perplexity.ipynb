{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_language_models_perplexity",
      "provenance": [],
      "authorship_tag": "ABX9TyOewJdvcKU2YNNNjOS807C3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basselkassem/nlp-toolkit/blob/master/4_language_models_perplexity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MjpxXuhzBaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import reuters\n",
        "from nltk import trigrams\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter, defaultdict\n",
        "from itertools import chain\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clQ87wIRzxa2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1d7a1b28-033a-4d36-dfb4-e4afef726aa8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxY3y9VxgEoI",
        "colab_type": "text"
      },
      "source": [
        "# ngram Language models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAwO8sl2jix3",
        "colab_type": "text"
      },
      "source": [
        "Language models assign a probabilites to the sequences: $word_1word_2...word_k$. Those models can be used to predict the next word given the previous context.\n",
        "\n",
        "Predicting the upcomming word, or giving a probability to a sentence is important in the following task:\n",
        "\n",
        "*   text completing / suggestions in messaging apps\n",
        "*   spelling correction\n",
        "*   machine translation(what is the most probable translation among all potential translations)\n",
        "\n",
        "**Build the model**\n",
        "\n",
        "$p(w_1, w_2, ..., w_k) = p(w_1)p(w_2|w_1)p(w_3|w_2,w_1)...p(w_k|w_{k-1},...w_1)$\n",
        "* add $<start>$ token and $<end>$ token to have a probability distribution\n",
        "* Markov assumption: $p(w_n|w_{k-1},...w_1) =p(w_n|w_{k-1})$\n",
        "* use ngram to serve as a context for the predicted word\n",
        "\n",
        "$p(w_1, w_2, ..., w_k) = \\prod_{i=1}^{k+1}p(w_i|w_{i-n + 1}^{i-1})$ where $w_{i-n + 1}^{i-1} = (w_{i-n+1}...w_{i-1})$ i.e the previous ngram \n",
        "\n",
        "**Training:**\n",
        "\n",
        "Training can be done using loglikehood estimation:\n",
        "\n",
        "$p(w_i|w_{i-n + 1}^{i-1}) = \\frac{\\#(w_i,w_{i-n + 1}^{i-1})}{\\#(w_{i-n + 1}^{i-1})}= \\frac{\\#(w_{i-n + 1}^{i})}{\\#(w_{i-n + 1}^{i-1})}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4cmLbCA2Nrj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3171e64f-658d-43e6-fed8-c9f8e17f6d2b"
      },
      "source": [
        "corpus =list(reuters.sents())\n",
        "np.random.shuffle(corpus)\n",
        "print(len(corpus))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "54716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWZ2c2Apzn-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "#count occurences of w3 given w2, w1\n",
        "for sentence in corpus:\n",
        "  for w1, w2, w3 in trigrams(sentence, pad_right = True, pad_left = True):\n",
        "    model[(w1, w2)][w3] += 1\n",
        "\n",
        "#normalize count of w3 given w2, w1 to probabilities\n",
        "for w1_w2 in model:\n",
        "  norm = float(sum(model[w1_w2].values()))\n",
        "  for w in model[w1_w2]:\n",
        "    model[w1_w2][w] /= norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WomrQCU7B7O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "a130d1b4-f91e-48a2-daf1-26bce9a60372"
      },
      "source": [
        "def generate_text(beginnig, model):\n",
        "  is_finished = False\n",
        "  text = beginnig.split()\n",
        "\n",
        "  while not is_finished:\n",
        "    w1_w2 = tuple(text[-2:])\n",
        "    thershold = np.random.random()\n",
        "    score = 0.0\n",
        "    for w3 in model[w1_w2].keys():\n",
        "      score += model[w1_w2][w3]\n",
        "      if score >= thershold:\n",
        "        text.append(w3)\n",
        "        break\n",
        "    if text[-2:] == [None, None]:\n",
        "      is_finished = True\n",
        "  return ' '.join(list(filter(None, text)))\n",
        "\n",
        "for i in range(10):\n",
        "  print(generate_text('money is', model))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "money is a leader in the quarter and 2 . 3 mln\n",
            "money is holding discussions with other debt .\n",
            "money is being carried at \" international markets , officials said .\n",
            "money is a major source of employment contracts with pay television networks said on his interest in a submission to the bankruptcy court ruling against it and Millipore intend to bring U . K ., West Germany .\n",
            "money is wasted .\n",
            "money is a political commitment to 275 dlrs per share .\n",
            "money is holding up well from now ?\"\n",
            "money is a rising trend in the index for input prices showed a surplus of 1 . 71 mln .\n",
            "money is a dramatic reversal in the U . S . Norstar operates in Canada ' s industry aid package that goes beyond the reach of our fiscal 1987 ' s posted price for the new division ' s central bank intervention to the International Cocoa Organization , ICO , passed over us ,\" he said .\n",
            "money is a private placement , which has ruled out offering to sell interests in some parts and recombining parts with other railroads , adds to the amount set aside for the shares .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D2XhNX3-lCg",
        "colab_type": "text"
      },
      "source": [
        "# Perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLXimT2I-qGu",
        "colab_type": "text"
      },
      "source": [
        "It is clear that bigger ngrams gives more accurate probabilites on the train set. But using bigger ngrams makes it harder to predict on the test set. In other words, it is more likely that the model will find a matching between 2-grams on the test set and the training set than finding a matching when n-gram = 7.\n",
        "\n",
        "Perplexity compute the quality of our language model by computing:\n",
        "\n",
        "$likelihood = p(w_{test}) = \\prod_{i=1}^{N +1}p(w_i|w_{i-n+1}^{i-1})$\n",
        "\n",
        "$perplexity = \\sqrt[N]\\frac{1}{p(w_{test})}$\n",
        "\n",
        "The smaller the perplexity, the better the model is"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vJJe5ErWc0W",
        "colab_type": "code",
        "outputId": "27ca4bde-6291-4801-8840-ec1c250a9dd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_txt = 'This is the rat that ate the malt that lay in the house that Jack built'\n",
        "test_txt = 'This is the house that Jack built'\n",
        "train_txt = train_txt.lower()\n",
        "test_txt = test_txt.lower()\n",
        "print(train_txt)\n",
        "print(test_txt)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this is the rat that ate the malt that lay in the house that jack built\n",
            "this is the house that jack built\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM6CdcRAWwKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_start_tokens(n_of_ngram, start_token = 'start'):\n",
        "  start_tokens = []\n",
        "  for i in range(n_of_ngram - 1):\n",
        "    start_tokens.append('<' + start_token + str(i + 1)  + '>')\n",
        "  return start_tokens\n",
        "\n",
        "def create_end_tokens(end_token = 'end'):\n",
        "  end_token = '<' + end_token + '>'\n",
        "  return end_token\n",
        "\n",
        "def pad_text(text, n_of_ngram, start_token = 'start', end_token = 'end'):\n",
        "  start_tokens = create_start_tokens(n_of_ngram, start_token)\n",
        "  end_token = create_end_tokens(end_token)\n",
        "  return ' '.join(start_tokens) + ' ' + text + ' ' + end_token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kjo6s9OW8Fv",
        "colab_type": "code",
        "outputId": "cce991ef-88de-4da7-8add-355a170e6e2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "n_of_ngram = 3\n",
        "train = pad_text(train_txt, n_of_ngram)\n",
        "test = pad_text(test_txt, n_of_ngram)\n",
        "print(train)\n",
        "print(test)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start1> <start2> this is the rat that ate the malt that lay in the house that jack built <end>\n",
            "<start1> <start2> this is the house that jack built <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg2O0j4YYIW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_ngrams(text, n_of_ngram):\n",
        "  n_grams = ngrams(text.split(), n_of_ngram)\n",
        "  return [' '.join(grams) for grams in  n_grams]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXqIDkb2Zizb",
        "colab_type": "code",
        "outputId": "68b04db7-5da0-4e1f-bde6-9b419943354e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_unigrams = create_ngrams(train, n_of_ngram = 1)\n",
        "test_unigrams = create_ngrams(test, n_of_ngram = 1)\n",
        "print(len(train_unigrams), len(test_unigrams))\n",
        "\n",
        "train_bigrams = create_ngrams(train, n_of_ngram = 2)\n",
        "test_bigrams = create_ngrams(test, n_of_ngram = 2)\n",
        "print(len(train_bigrams), len(test_bigrams))\n",
        "\n",
        "train_trigrams = create_ngrams(train, n_of_ngram = 3)\n",
        "test_trigrams = create_ngrams(test, n_of_ngram = 3)\n",
        "print(len(train_trigrams), len(test_trigrams))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19 10\n",
            "18 9\n",
            "17 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXy680R1agW9",
        "colab_type": "code",
        "outputId": "eea2e5ec-63a2-4c50-c8a0-f0ee28c040ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "unique_train_unigrams = set(train_unigrams)\n",
        "start_tokens = create_start_tokens(n_of_ngram)\n",
        "for token in start_tokens:\n",
        "  unique_train_unigrams -= {token}\n",
        "vocab_size = len(unique_train_unigrams)\n",
        "N_test = len(test_txt.split())\n",
        "print(vocab_size, N_test)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir2B-GKtftYg",
        "colab_type": "text"
      },
      "source": [
        "# Build language model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUzfZXiWfxMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigrams_count = Counter(train_unigrams)\n",
        "bigrams_count = Counter(train_bigrams)\n",
        "trigrams_count = Counter(train_trigrams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REWrRma74y7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_liklehood(ngrams, n_of_ngram):\n",
        "  liklehood = 1\n",
        "  for wi_wi_1_wi_2 in ngrams:\n",
        "    tokens = wi_wi_1_wi_2.split()\n",
        "    wi_1_wi_2 = ' '.join(tokens[: -(n_of_ngram - 1) + 1])\n",
        "    nominator = trigrams_count[wi_wi_1_wi_2]\n",
        "    dominator = bigrams_count[wi_1_wi_2]\n",
        "    liklehood *=  nominator / dominator\n",
        "  return liklehood"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyTmGrAa4zLe",
        "colab_type": "code",
        "outputId": "0c1a654d-213f-4f78-de1f-dc40fd0d6970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "liklehood = compute_liklehood(test_trigrams, n_of_ngram)\n",
        "print(liklehood)\n",
        "#print(compute_perplexity(liklehood))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tLg4ELydx7g",
        "colab_type": "text"
      },
      "source": [
        "# add-one-smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y8rhJq4no9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_liklehood_add_one(ngrams, n_of_ngram):\n",
        "  liklehood = 1\n",
        "  for wi_wi_1_wi_2 in ngrams:\n",
        "    tokens = wi_wi_1_wi_2.split()\n",
        "    wi_1_i_2 = ' '.join(tokens[: -(n_of_ngram - 1) + 1])\n",
        "\n",
        "    nominator = trigrams_count[wi_wi_1_wi_2] + 1\n",
        "    dominator = bigrams_count[wi_1_i_2] + vocab_size\n",
        "    liklehood *=  nominator / dominator\n",
        "  return liklehood\n",
        "\n",
        "def compute_perplexity(liklehood):\n",
        "  return np.power(liklehood, -1 / N_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsTqWzi-bc5V",
        "colab_type": "code",
        "outputId": "4868aebf-d09b-4c53-92a8-1a894df35c14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "liklehood = compute_liklehood_add_one(test_trigrams, n_of_ngram)\n",
        "perplexity = compute_perplexity(liklehood)\n",
        "print(perplexity)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.205413747033983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4FOu-aYuWo1",
        "colab_type": "text"
      },
      "source": [
        "# interpolate-smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u54TvYZKvdv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_liklehood_interpolate(ngrams, n_of_ngram):\n",
        "  lamb1 = lamb2 = lamb3 = 1.0 / 3\n",
        "  assert lamb1 + lamb2 + lamb3 == 1\n",
        "  liklehood = 1\n",
        "  for wi_2_wi_1_wi in ngrams:\n",
        "    tokens = wi_2_wi_1_wi.split()\n",
        "    # p(wi given wi-2, wi-1)\n",
        "    wi_2_wi_1 = ' '.join(tokens[: -(n_of_ngram - 1) + 1])\n",
        "    p_wi_given_wi_2_wi_1 = lamb1 * trigrams_count[wi_2_wi_1_wi] / bigrams_count[wi_2_wi_1]\n",
        "\n",
        "    # p(wi given wi-1)\n",
        "    wi_1 = ' '.join(tokens[-(n_of_ngram - 1): -(n_of_ngram - 1) + 1])\n",
        "    wi_wi_1 = ' '.join(tokens[-(n_of_ngram - 1): ])\n",
        "    p_wi_given_wi_1 = lamb2 * bigrams_count[wi_wi_1] / unigrams_count[wi_1]\n",
        "\n",
        "    # p(wi)\n",
        "    wi = tokens[-1]\n",
        "    p_wi = lamb3 * 1 / unigrams_count[wi]\n",
        "\n",
        "    p_hat = p_wi_given_wi_2_wi_1 + p_wi_given_wi_1 + p_wi\n",
        "    liklehood *= p_hat\n",
        "  return liklehood\n",
        "liklehood = compute_liklehood_interpolate(test_trigrams, n_of_ngram)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbb2upGpbgNo",
        "colab_type": "code",
        "outputId": "934f245c-9814-4f6f-a573-bf87ea5c990d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "perplexity = compute_perplexity(liklehood)\n",
        "print(perplexity)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.2505123624523526\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iC7EggT2bQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
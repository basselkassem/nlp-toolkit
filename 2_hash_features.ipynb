{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_hash_features.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP5qPDBYflI53E0wsJPBzJS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basselkassem/nlp-toolkit/blob/master/2_hash_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOp8ogAfnavx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs = [\n",
        "        'it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory',\n",
        "        'it is fast to pickle and un-pickle as it holds no state besides the constructor parameters',\n",
        "        'it can be used in a streaming (partial fit) or parallel pipeline as there is no state computed during fit.'\n",
        "]\n",
        "users = ['user1', 'user2', 'user1']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCaYLY3HCHO8",
        "colab_type": "text"
      },
      "source": [
        "#Hash encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmETgum7i0P8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "When we have large text data, it becomes not possible to represent those texts with bag of words or tfidf. \n",
        "An alternative solution is to compute the hash of the tokens/n-grams.\n",
        "\n",
        "**Example: Spam filtering**\n",
        "\n",
        "\n",
        "*   0.4 million user\n",
        "*   3.2 million emails\n",
        "*   40 million unique token\n",
        "\n",
        "We map each token/n-gram to an index(number) by computing the hash function like: $\\phi(x)=hash(x)\\% 2^b$, if b = 22 we have 4 million features.\n",
        "\n",
        "Hash functions introduce collisions, but it is proven through [experiments](https://arxiv.org/pdf/0902.2206.pdf) that those collisions do not reduces the quality of the model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijWymuMUinbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP_Z0ZT4neq-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26dbd976-e498-4217-cfc7-d12b478ba5cf"
      },
      "source": [
        "vectorizer = HashingVectorizer(analyzer='word', stop_words='english', ngram_range=[1, 1], n_features=10)\n",
        "X = vectorizer.fit_transform(docs).toarray()\n",
        "X.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51Luf1yBsZbh",
        "colab_type": "text"
      },
      "source": [
        "## Personalized token triks\n",
        "\n",
        "Map each user to the tokens. In other words, emails that are considered a spam by some user are not considered spams by another. In order to learn this preference, we do:\n",
        "\n",
        "\n",
        "*   $\\phi_o(token)=hash(token)\\%2^b$\n",
        "*   $\\phi_u(user +''\\_''+token) = hash(user +''\\_''+token) \\%2^b$\n",
        "*   $\\phi(user, token) = \\phi_o(token) + \\phi_u(user +''\\_''+token)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bdlwudnsLH3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "5faa8862-a8ba-4700-bf36-a7beabca5dc6"
      },
      "source": [
        "users_docs = []\n",
        "for i, user in enumerate(users):\n",
        "  user_doc = [user + '_' + token for token in docs[i].split()]\n",
        "  users_docs.append(' '.join(user_doc))\n",
        "users_docs"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['user1_it user1_is user1_very user1_low user1_memory user1_scalable user1_to user1_large user1_datasets user1_as user1_there user1_is user1_no user1_need user1_to user1_store user1_a user1_vocabulary user1_dictionary user1_in user1_memory',\n",
              " 'user2_it user2_is user2_fast user2_to user2_pickle user2_and user2_un-pickle user2_as user2_it user2_holds user2_no user2_state user2_besides user2_the user2_constructor user2_parameters',\n",
              " 'user1_it user1_can user1_be user1_used user1_in user1_a user1_streaming user1_(partial user1_fit) user1_or user1_parallel user1_pipeline user1_as user1_there user1_is user1_no user1_state user1_computed user1_during user1_fit.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFMdaFKqvkNI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a698cd89-7012-4592-f914-5fa25e43c258"
      },
      "source": [
        "vectorizer = HashingVectorizer(analyzer='word', stop_words='english', ngram_range=[1, 1], n_features=10)\n",
        "phi_o = vectorizer.fit_transform(docs).toarray()\n",
        "phi_u = vectorizer.fit_transform(users_docs).toarray()\n",
        "X = phi_o + phi_u\n",
        "X.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}